{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project-2-haoyuzhang89_cloudera_1 is up-to-date\n",
      "project-2-haoyuzhang89_zookeeper_1 is up-to-date\n",
      "project-2-haoyuzhang89_mids_1 is up-to-date\n",
      "project-2-haoyuzhang89_spark_1 is up-to-date\n",
      "project-2-haoyuzhang89_kafka_1 is up-to-date\n"
     ]
    }
   ],
   "source": [
    "! docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name                   Command           State           Ports         \n",
      "--------------------------------------------------------------------------------\n",
      "project-2-haoyuzhang89   cdh_startup_script.sh    Up      11000/tcp, 11443/tcp, \n",
      "_cloudera_1                                               19888/tcp, 50070/tcp, \n",
      "                                                          8020/tcp, 8088/tcp,   \n",
      "                                                          8888/tcp, 9090/tcp    \n",
      "project-2-haoyuzhang89   /etc/confluent/docker/   Up      29092/tcp, 9092/tcp   \n",
      "_kafka_1                 run                                                    \n",
      "project-2-haoyuzhang89   /bin/bash                Up      8888/tcp              \n",
      "_mids_1                                                                         \n",
      "project-2-haoyuzhang89   docker-entrypoint.sh     Up      0.0.0.0:8888->8888/tcp\n",
      "_spark_1                 bash                                                   \n",
      "project-2-haoyuzhang89   /etc/confluent/docker/   Up      2181/tcp, 2888/tcp,   \n",
      "_zookeeper_1             run                              32181/tcp, 3888/tcp   \n"
     ]
    }
   ],
   "source": [
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic assessment.\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessment \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: assessment\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: assessment\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --describe \\\n",
    "    --topic assessment \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker-compose exec mids bash -c \"cat project-2-haoyuzhang89/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessment -o beginning -e \"|wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"keen_timestamp\":\"1513766763.6051481\",\"max_attempts\":\"1.0\",\"started_at\":\"2017-12-20T10:44:09.162Z\",\"base_exam_id\":\"f80366d9-db60-41c3-a1c4-6c7789b478f8\",\"user_exam_id\":\"b1896278-669f-4346-80fd-21d0ba898d5d\",\"sequences\":{\"questions\":[{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"2e31babb-5a1c-47bd-bcb7-d4fa3d43794f\"},{\"checked\":true,\"at\":\"2017-12-20T10:44:53.863Z\",\"id\":\"68e4f6aa-2adb-4402-b5b7-b7610993edc6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:44:49.319Z\",\"id\":\"bbe2135d-cc21-493b-b4e1-8a182e6211d4\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:44:37.080Z\",\"id\":\"feba554f-1b5e-422d-93ac-b9202a91014b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bf8306dd-889f-4e10-b305-ef446fa4cec4\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"17d6282d-8c2f-4d67-b15e-c563b38c6a99\"},{\"checked\":true,\"at\":\"2017-12-20T10:45:10.020Z\",\"id\":\"5ab90b76-f93a-4078-9b98-81360d8859d4\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"0809acb6-dbbb-4c75-8d9f-9ee7e48425e1\"},{\"checked\":true,\"at\":\"2017-12-20T10:45:12.309Z\",\"id\":\"cf3e899e-d8c1-4644-b10f-3a57b1bab5ea\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"4c64e2b7-e503-4f81-8441-866a0c04aafa\",\"user_result\":\"correct\"},{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:45:21.474Z\",\"id\":\"47b50665-2de5-4dff-bc30-fccf8d4a94e6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:29.661Z\",\"id\":\"052c77b9-c6c7-4776-bfda-f68e847d8c8a\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"889151e0-6cd3-4f9e-a32d-d48cdf2ef967\"},{\"checked\":false,\"id\":\"f03b7edf-240a-47bb-aeb3-692132a6612c\"}],\"user_submitted\":true,\"id\":\"bf53ec88-5551-49dd-86c6-39cc7bb3930f\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:45:50.794Z\",\"id\":\"c0c7e8ca-cfd4-42c9-bfe2-2cb2d67440d6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:52.451Z\",\"id\":\"1a9c20d8-fd9a-4542-8b79-72cddfa6ea6c\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:54.825Z\",\"id\":\"d2f7abbc-b92e-4d52-a6fc-351e31e3d3f1\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:46:01.361Z\",\"id\":\"e92390a5-4d59-4969-a196-df043457d354\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"2e1aee4a-bd0a-41bd-acd5-32145796813f\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"655fa8c5-3703-42c4-a99e-0296326dc087\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":0,\"all_correct\":false,\"correct\":3,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1513766763.6051481\",\"certification\":\"false\",\"keen_id\":\"5a3a3f6b8c7fa00001d2a03e\",\"exam_name\":\"I'm a Software Architect, Now What?\"}\n",
      "{\"keen_timestamp\":\"1513766386.5845051\",\"max_attempts\":\"1.0\",\"started_at\":\"2017-12-20T10:38:26.490Z\",\"base_exam_id\":\"a62e5d35-75e9-11e6-8197-9801a7c3b233\",\"user_exam_id\":\"3a1d33b9-b79c-42b3-99a8-1ce4a4291bf2\",\"sequences\":{\"questions\":[{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:38:29.540Z\",\"id\":\"a8b3951b-d1e1-4fa7-94cb-0a0415be1573\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"f026267d-1997-48a3-9021-5c27fb743f3b\"},{\"checked\":false,\"id\":\"72f87cf2-b723-4a02-b5b6-538278349eab\"},{\"checked\":false,\"id\":\"b64ddac5-5ac6-431a-b73d-c7388c760b85\"}],\"user_submitted\":true,\"id\":\"ff4fd2c3-d00d-4fee-b783-f1a6091f61f9\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"2b4176a1-3492-42e8-b4fe-433d27b9662a\"},{\"checked\":false,\"id\":\"7ab55774-61bf-43f4-8a35-764d02a83c8b\"},{\"checked\":false,\"id\":\"e82d47d4-6498-41e4-8dce-5e78ea17cf5b\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:31.451Z\",\"id\":\"97864885-ce7e-4a12-b129-b8d2c6f80928\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"02fa4d4d-f0e8-4e6b-997c-3b823acef6bb\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"41877b0a-ee7b-4d3d-8c89-6f5826e79fda\"},{\"checked\":false,\"id\":\"e7f1863a-a78d-4f29-8ac7-1d3012477eae\"},{\"checked\":false,\"id\":\"45c437e7-4e95-42aa-b83a-b09acef38b6d\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:39.047Z\",\"id\":\"5a6dbb51-692b-4914-a620-afe6af303916\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"9c08dc92-611f-4211-a290-956c316d7291\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"4715b39d-0a3a-42d3-893a-494ceb935e6c\"},{\"checked\":false,\"id\":\"2b394228-a8b5-4b10-87c9-53f90b42c6f1\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:52.610Z\",\"id\":\"e8e0558f-a379-4e30-b615-128c1ea43423\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"d202ad94-17a2-404d-b0de-8a8fd43e9b8c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"a970d87f-afc4-4e0f-a84d-edc8ae1d82f0\"},{\"checked\":false,\"id\":\"fbdba43b-3986-434d-b9b9-353530d54c18\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:56.685Z\",\"id\":\"9e93bcac-b886-413c-b406-109c3bbed4ed\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:38:57.399Z\",\"id\":\"f15e504b-67f3-4c83-9860-53f91174658d\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"68e2ea30-6e5f-4d9a-b044-3ed5e53dca3e\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"5a4c78cc-0239-44b6-b871-dd8e47f9b826\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:04.501Z\",\"id\":\"fa6a8c7e-7d11-4b72-9214-9e0d139789a7\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"035db32c-8feb-4a6c-a1f7-68f5156a6d8c\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:06.721Z\",\"id\":\"52209cb9-c7a1-49b7-aba3-d49e229f4479\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"3a57fcba-070f-4f9c-9c69-a398ce32366f\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:09.877Z\",\"id\":\"13f07602-478b-4acb-b2b1-d219a8fddc9a\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"4d5cf201-976d-4da8-8c34-e6e06dc426cf\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:39:36.052Z\",\"id\":\"11bde835-5ab0-4694-b05b-8fdf67206a5f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"at\":\"2017-12-20T10:39:35.112Z\",\"id\":\"e6e5abdd-98a0-487b-a8da-0fbf3125a023\"},{\"checked\":false,\"correct\":true,\"id\":\"18f0c4ba-89de-474b-8e3b-b220a663f43a\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:27.147Z\",\"id\":\"634020ce-f232-4ee3-859b-f5e87e21857b\",\"submitted\":1}],\"user_submitted\":true,\"id\":\"6a776eb4-b26d-4ba9-a41f-37153086bba8\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:39:30.695Z\",\"id\":\"c4ec70ac-0407-4dda-ace7-2cf751d76464\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"dbca7b8a-a4fd-4a75-8221-6c236e14a86b\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:42.412Z\",\"id\":\"a3b69e1d-b7c2-4efd-a822-30adf80a5ac3\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"5ee97047-af1c-4206-a3b8-af34675e2fea\"}],\"user_submitted\":true,\"id\":\"1ebe9acf-617c-44d3-8c23-e5815ab108bf\",\"user_result\":\"incorrect\"}],\"attempt\":1,\"id\":\"7948e155-ffbb-4179-84af-7e358767a750\",\"counts\":{\"incomplete\":0,\"submitted\":8,\"incorrect\":2,\"all_correct\":false,\"correct\":6,\"total\":8,\"unanswered\":0}},\"keen_created_at\":\"1513766386.5845051\",\"certification\":\"false\",\"keen_id\":\"5a3a3df2448eb200012a2efd\",\"exam_name\":\"Learning Linux System Administration\"}\n",
      "% Reached end of topic assessment [0] at offset 3280: exiting\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessment -o 3278 -e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Jupyter pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 15:51:11.390 NotebookApp]\u001b(B\u001b[m Serving notebooks from local directory: /w205\n",
      "\u001b[32m[I 15:51:11.391 NotebookApp]\u001b(B\u001b[m 0 active kernels \n",
      "\u001b[32m[I 15:51:11.392 NotebookApp]\u001b(B\u001b[m The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=5c372c1003023d3bde0bf7a5d424e6157b4a9a0855cab16c\n",
      "\u001b[32m[I 15:51:11.392 NotebookApp]\u001b(B\u001b[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[C 15:51:11.394 NotebookApp] \n",
      "    \n",
      "    Copy/paste this URL into your browser when you connect for the first time,\n",
      "    to login with a token:\n",
      "        http://0.0.0.0:8888/?token=5c372c1003023d3bde0bf7a5d424e6157b4a9a0855cab16c\n",
      "\u001b[32m[I 15:51:49.001 NotebookApp]\u001b(B\u001b[m 302 GET /?token=5c372c1003023d3bde0bf7a5d424e6157b4a9a0855cab16c (198.54.105.27) 0.72ms\n",
      "\u001b[32m[I 15:51:59.086 NotebookApp]\u001b(B\u001b[m Kernel started: c52e5865-e9fc-43fc-9ae2-45f9051a62d1\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\u001b[33m[W 15:52:09.249 NotebookApp]\u001b(B\u001b[m Timeout waiting for kernel_info reply from c52e5865-e9fc-43fc-9ae2-45f9051a62d1\n",
      "20/10/24 15:52:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/10/24 15:52:23 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "\u001b[32m[I 15:53:59.078 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 15:59:59.079 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:01:59.098 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:03:59.139 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:05:59.626 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "20/10/24 16:06:36 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"<ipython-input-8-6041f099f0f4>\", line 1, in <lambda>\n",
      "NameError: name 'json' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:06:36 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"<ipython-input-8-6041f099f0f4>\", line 1, in <lambda>\n",
      "NameError: name 'json' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:06:36 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n",
      "\u001b[32m[I 16:07:59.098 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "20/10/24 16:08:14 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"<ipython-input-15-abbf0cdd5c30>\", line 1, in <lambda>\n",
      "NameError: name 'json' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:08:14 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"<ipython-input-15-abbf0cdd5c30>\", line 1, in <lambda>\n",
      "NameError: name 'json' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:08:14 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n",
      "\u001b[32m[I 16:09:59.087 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:11:59.071 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:13:59.090 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:15:59.090 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:21:59.089 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:23:59.080 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:29:59.171 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:31:59.088 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[33m[W 16:33:02.558 NotebookApp]\u001b(B\u001b[m 404 GET /static/components/preact/preact.min.js.map (198.54.105.27) 21.51ms referer=None\n",
      "\u001b[33m[W 16:33:02.596 NotebookApp]\u001b(B\u001b[m 404 GET /static/components/proptypes/index.js.map (198.54.105.27) 2.80ms referer=None\n",
      "\u001b[33m[W 16:33:02.603 NotebookApp]\u001b(B\u001b[m 404 GET /static/components/preact-compat/preact-compat.min.js.map (198.54.105.27) 2.95ms referer=None\n",
      "\u001b[32m[I 16:33:59.104 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:35:59.082 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:37:59.085 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:39:59.082 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:41:59.084 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:43:59.088 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:45:59.088 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:47:59.647 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:49:59.095 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:51:59.085 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:53:59.086 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "20/10/24 16:54:02 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:54:02 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:54:02 ERROR TaskSetManager: Task 0 in stage 11.0 failed 1 times; aborting job\n",
      "20/10/24 16:55:37 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 12)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:55:37 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 12, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:55:37 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job\n",
      "\u001b[32m[I 16:55:59.095 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "20/10/24 16:56:37 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 14)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:56:37 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 14, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:56:37 ERROR TaskSetManager: Task 0 in stage 14.0 failed 1 times; aborting job\n",
      "20/10/24 16:56:43 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 15)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/10/24 16:56:43 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 15, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n",
      "    process()\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py\", line 520, in prepare\n",
      "    verify_func(obj, schema)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1350, in _verify_type\n",
      "    _verify_type(obj.get(f.name), f.dataType, f.nullable)\n",
      "  File \"/spark-2.2.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1324, in _verify_type\n",
      "    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\n",
      "TypeError: TimestampType can not accept object '1516717442.735266' in type <class 'str'>\n",
      "\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
      "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/10/24 16:56:43 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job\n",
      "\u001b[32m[I 16:57:59.105 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:58:39.232 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 16:59:59.095 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:01:59.101 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:03:59.687 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:05:59.361 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:07:59.349 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:09:02.216 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:09:59.136 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:11:24.950 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:11:59.216 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:13:59.161 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:15:59.207 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:17:59.125 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:19:59.111 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:21:59.109 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:22:33.877 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:22:37.182 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n",
      "\u001b[32m[I 17:22:42.844 NotebookApp]\u001b(B\u001b[m Saving file at /project-2-haoyuzhang89/Project_2-Pyspark.ipynb\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root --notebook-dir=/w205/' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Here are some example questions that would be useful to know, but again, you choose your own (1 to 3).These examples are meant to get you started/thinking, they are optional.\n",
    "\n",
    "- How many assesstments are in the dataset?\n",
    "\n",
    "- What's the name of your Kafka topic? How did you come up with that name?\n",
    "\n",
    "- How many people took Learning Git?\n",
    "\n",
    "- What is the least common course taken? And the most common?\n",
    "\n",
    "- Add any query(ies) you think will help the data science team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
