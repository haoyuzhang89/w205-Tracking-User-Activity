{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this project, a service that delivers assessments from an education tech firm was created. The data outcome is ready for further queries work according to customer's requirements. \n",
    "\n",
    "Main tasks of this project\n",
    "\n",
    "- Publish and consume messages with Kafka\n",
    "- Use Spark to transform the messages. \n",
    "- Use Spark to transform the messages so that can be landed in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data & Docker Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The original data was acquired by running \n",
    "\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
    "```\n",
    "\n",
    "- The data is contained in the file `assessment-attempts-20180128-121051-nested.json`, which is inclued in this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuration of application service  is  defined in `docker-compose.yml`, which is included in this repository. Docker images of cloudera, kafka, mids, spark, zookeeper was used in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting project-2-haoyuzhang89_cloudera_1 ... \n",
      "Starting project-2-haoyuzhang89_zookeeper_1 ... \n",
      "Starting project-2-haoyuzhang89_mids_1      ... \n",
      "\u001b[3BStarting project-2-haoyuzhang89_spark_1     ... mdone\u001b[0m\n",
      "\u001b[3BStarting project-2-haoyuzhang89_kafka_1     ... mdone\u001b[0m\u001b[3A\u001b[2K\n",
      "\u001b[2Bting project-2-haoyuzhang89_spark_1     ... \u001b[32mdone\u001b[0m\u001b[2A\u001b[2K"
     ]
    }
   ],
   "source": [
    "! docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State list of Docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name                   Command           State           Ports         \n",
      "--------------------------------------------------------------------------------\n",
      "project-2-haoyuzhang89   cdh_startup_script.sh    Up      11000/tcp, 11443/tcp, \n",
      "_cloudera_1                                               19888/tcp, 50070/tcp, \n",
      "                                                          8020/tcp, 8088/tcp,   \n",
      "                                                          8888/tcp, 9090/tcp    \n",
      "project-2-haoyuzhang89   /etc/confluent/docker/   Up      29092/tcp, 9092/tcp   \n",
      "_kafka_1                 run                                                    \n",
      "project-2-haoyuzhang89   /bin/bash                Up      8888/tcp              \n",
      "_mids_1                                                                         \n",
      "project-2-haoyuzhang89   docker-entrypoint.sh     Up      0.0.0.0:8888->8888/tcp\n",
      "_spark_1                 bash                                                   \n",
      "project-2-haoyuzhang89   /etc/confluent/docker/   Up      2181/tcp, 2888/tcp,   \n",
      "_zookeeper_1             run                              32181/tcp, 3888/tcp   \n"
     ]
    }
   ],
   "source": [
    "! docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Publish & Consume with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Public Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The kafka topic is named as __assessment__ since the json file containing assessment results of users from an education tech firm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessment \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description of the kafka topic __assessment__ is listed to check if the topic was created successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: assessment\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
      "\tTopic: assessment\tPartition: 0\tLeader: 1\tReplicas: 1\tIsr: 1\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --describe \\\n",
    "    --topic assessment \\\n",
    "    --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The assessment information of `assessment-attempts-20180128-121051-nested.json` was published into the kafka topic __assessment__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker-compose exec mids bash -c \"cat project-2-haoyuzhang89/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Consume Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read mesages from kafka and count lines number of the messages. It shows that there is 3281 lines of messages contained in the kafka topic __assessment__ by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessment -o beginning -e \"|wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load messages from kafka to display final lines of the messages. Curretnly, messages contained in the kafka topic is barely organized. It is difficult to interprete useful information from it right now. However, it shows that the messages publish work was accomplished successfully in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"keen_timestamp\":\"1513766763.6051481\",\"max_attempts\":\"1.0\",\"started_at\":\"2017-12-20T10:44:09.162Z\",\"base_exam_id\":\"f80366d9-db60-41c3-a1c4-6c7789b478f8\",\"user_exam_id\":\"b1896278-669f-4346-80fd-21d0ba898d5d\",\"sequences\":{\"questions\":[{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"2e31babb-5a1c-47bd-bcb7-d4fa3d43794f\"},{\"checked\":true,\"at\":\"2017-12-20T10:44:53.863Z\",\"id\":\"68e4f6aa-2adb-4402-b5b7-b7610993edc6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:44:49.319Z\",\"id\":\"bbe2135d-cc21-493b-b4e1-8a182e6211d4\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:44:37.080Z\",\"id\":\"feba554f-1b5e-422d-93ac-b9202a91014b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bf8306dd-889f-4e10-b305-ef446fa4cec4\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"17d6282d-8c2f-4d67-b15e-c563b38c6a99\"},{\"checked\":true,\"at\":\"2017-12-20T10:45:10.020Z\",\"id\":\"5ab90b76-f93a-4078-9b98-81360d8859d4\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"0809acb6-dbbb-4c75-8d9f-9ee7e48425e1\"},{\"checked\":true,\"at\":\"2017-12-20T10:45:12.309Z\",\"id\":\"cf3e899e-d8c1-4644-b10f-3a57b1bab5ea\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"4c64e2b7-e503-4f81-8441-866a0c04aafa\",\"user_result\":\"correct\"},{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:45:21.474Z\",\"id\":\"47b50665-2de5-4dff-bc30-fccf8d4a94e6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:29.661Z\",\"id\":\"052c77b9-c6c7-4776-bfda-f68e847d8c8a\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"889151e0-6cd3-4f9e-a32d-d48cdf2ef967\"},{\"checked\":false,\"id\":\"f03b7edf-240a-47bb-aeb3-692132a6612c\"}],\"user_submitted\":true,\"id\":\"bf53ec88-5551-49dd-86c6-39cc7bb3930f\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:45:50.794Z\",\"id\":\"c0c7e8ca-cfd4-42c9-bfe2-2cb2d67440d6\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:52.451Z\",\"id\":\"1a9c20d8-fd9a-4542-8b79-72cddfa6ea6c\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:45:54.825Z\",\"id\":\"d2f7abbc-b92e-4d52-a6fc-351e31e3d3f1\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:46:01.361Z\",\"id\":\"e92390a5-4d59-4969-a196-df043457d354\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"2e1aee4a-bd0a-41bd-acd5-32145796813f\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"655fa8c5-3703-42c4-a99e-0296326dc087\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":0,\"all_correct\":false,\"correct\":3,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1513766763.6051481\",\"certification\":\"false\",\"keen_id\":\"5a3a3f6b8c7fa00001d2a03e\",\"exam_name\":\"I'm a Software Architect, Now What?\"}\n",
      "{\"keen_timestamp\":\"1513766386.5845051\",\"max_attempts\":\"1.0\",\"started_at\":\"2017-12-20T10:38:26.490Z\",\"base_exam_id\":\"a62e5d35-75e9-11e6-8197-9801a7c3b233\",\"user_exam_id\":\"3a1d33b9-b79c-42b3-99a8-1ce4a4291bf2\",\"sequences\":{\"questions\":[{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:38:29.540Z\",\"id\":\"a8b3951b-d1e1-4fa7-94cb-0a0415be1573\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"f026267d-1997-48a3-9021-5c27fb743f3b\"},{\"checked\":false,\"id\":\"72f87cf2-b723-4a02-b5b6-538278349eab\"},{\"checked\":false,\"id\":\"b64ddac5-5ac6-431a-b73d-c7388c760b85\"}],\"user_submitted\":true,\"id\":\"ff4fd2c3-d00d-4fee-b783-f1a6091f61f9\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"2b4176a1-3492-42e8-b4fe-433d27b9662a\"},{\"checked\":false,\"id\":\"7ab55774-61bf-43f4-8a35-764d02a83c8b\"},{\"checked\":false,\"id\":\"e82d47d4-6498-41e4-8dce-5e78ea17cf5b\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:31.451Z\",\"id\":\"97864885-ce7e-4a12-b129-b8d2c6f80928\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"02fa4d4d-f0e8-4e6b-997c-3b823acef6bb\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"41877b0a-ee7b-4d3d-8c89-6f5826e79fda\"},{\"checked\":false,\"id\":\"e7f1863a-a78d-4f29-8ac7-1d3012477eae\"},{\"checked\":false,\"id\":\"45c437e7-4e95-42aa-b83a-b09acef38b6d\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:39.047Z\",\"id\":\"5a6dbb51-692b-4914-a620-afe6af303916\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"9c08dc92-611f-4211-a290-956c316d7291\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"4715b39d-0a3a-42d3-893a-494ceb935e6c\"},{\"checked\":false,\"id\":\"2b394228-a8b5-4b10-87c9-53f90b42c6f1\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:52.610Z\",\"id\":\"e8e0558f-a379-4e30-b615-128c1ea43423\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"d202ad94-17a2-404d-b0de-8a8fd43e9b8c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"a970d87f-afc4-4e0f-a84d-edc8ae1d82f0\"},{\"checked\":false,\"id\":\"fbdba43b-3986-434d-b9b9-353530d54c18\"},{\"checked\":true,\"at\":\"2017-12-20T10:38:56.685Z\",\"id\":\"9e93bcac-b886-413c-b406-109c3bbed4ed\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2017-12-20T10:38:57.399Z\",\"id\":\"f15e504b-67f3-4c83-9860-53f91174658d\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"68e2ea30-6e5f-4d9a-b044-3ed5e53dca3e\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"5a4c78cc-0239-44b6-b871-dd8e47f9b826\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:04.501Z\",\"id\":\"fa6a8c7e-7d11-4b72-9214-9e0d139789a7\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"035db32c-8feb-4a6c-a1f7-68f5156a6d8c\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:06.721Z\",\"id\":\"52209cb9-c7a1-49b7-aba3-d49e229f4479\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"3a57fcba-070f-4f9c-9c69-a398ce32366f\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:09.877Z\",\"id\":\"13f07602-478b-4acb-b2b1-d219a8fddc9a\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"4d5cf201-976d-4da8-8c34-e6e06dc426cf\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:39:36.052Z\",\"id\":\"11bde835-5ab0-4694-b05b-8fdf67206a5f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"at\":\"2017-12-20T10:39:35.112Z\",\"id\":\"e6e5abdd-98a0-487b-a8da-0fbf3125a023\"},{\"checked\":false,\"correct\":true,\"id\":\"18f0c4ba-89de-474b-8e3b-b220a663f43a\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:27.147Z\",\"id\":\"634020ce-f232-4ee3-859b-f5e87e21857b\",\"submitted\":1}],\"user_submitted\":true,\"id\":\"6a776eb4-b26d-4ba9-a41f-37153086bba8\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2017-12-20T10:39:30.695Z\",\"id\":\"c4ec70ac-0407-4dda-ace7-2cf751d76464\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"id\":\"dbca7b8a-a4fd-4a75-8221-6c236e14a86b\"},{\"checked\":true,\"at\":\"2017-12-20T10:39:42.412Z\",\"id\":\"a3b69e1d-b7c2-4efd-a822-30adf80a5ac3\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"5ee97047-af1c-4206-a3b8-af34675e2fea\"}],\"user_submitted\":true,\"id\":\"1ebe9acf-617c-44d3-8c23-e5815ab108bf\",\"user_result\":\"incorrect\"}],\"attempt\":1,\"id\":\"7948e155-ffbb-4179-84af-7e358767a750\",\"counts\":{\"incomplete\":0,\"submitted\":8,\"incorrect\":2,\"all_correct\":false,\"correct\":6,\"total\":8,\"unanswered\":0}},\"keen_created_at\":\"1513766386.5845051\",\"certification\":\"false\",\"keen_id\":\"5a3a3df2448eb200012a2efd\",\"exam_name\":\"Learning Linux System Administration\"}\n",
      "% Reached end of topic assessment [0] at offset 3280: exiting\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessment -o 3278 -e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tranform in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pyspark Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The spark transform is operated by pyspark driver in a jupyter notebook. Running the spark driver in a jupyter notebook environment as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 21:43:02.862 NotebookApp]\u001b(B\u001b[m Serving notebooks from local directory: /w205\n",
      "\u001b[32m[I 21:43:02.862 NotebookApp]\u001b(B\u001b[m 0 active kernels \n",
      "\u001b[32m[I 21:43:02.863 NotebookApp]\u001b(B\u001b[m The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=89558d308879801675a8098e2939216f6ff3a2967764ac34\n",
      "\u001b[32m[I 21:43:02.863 NotebookApp]\u001b(B\u001b[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
      "[C 21:43:02.865 NotebookApp] \n",
      "    \n",
      "    Copy/paste this URL into your browser when you connect for the first time,\n",
      "    to login with a token:\n",
      "        http://0.0.0.0:8888/?token=89558d308879801675a8098e2939216f6ff3a2967764ac34\n",
      "\u001b[32m[I 21:43:50.385 NotebookApp]\u001b(B\u001b[m 302 GET /?token=89558d308879801675a8098e2939216f6ff3a2967764ac34 (198.54.105.27) 0.81ms\n",
      "\u001b[33m[W 21:43:56.231 NotebookApp]\u001b(B\u001b[m Notebook project-2-haoyuzhang89/Project_2.ipynb is not trusted\n",
      "\u001b[32m[I 21:43:57.467 NotebookApp]\u001b(B\u001b[m Kernel started: 59a8c130-93e5-423b-95b5-abf465e03c39\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\u001b[33m[W 21:44:07.721 NotebookApp]\u001b(B\u001b[m Timeout waiting for kernel_info reply from 59a8c130-93e5-423b-95b5-abf465e03c39\n",
      "20/10/25 21:44:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/10/25 21:44:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root --notebook-dir=/w205/' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading packages required for futher transfor work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check out Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2020-10-24 21:38 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Read from Kafka topic: assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the raw assessment messages from kafka topic __assessment__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_assessment = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessment\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cache the raw assessment messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessment.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the schema of the raw assessment messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessment.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cast the raw assessment data as string type. Then save the string data in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment = raw_assessment.select(raw_assessment.value.cast('string'))\n",
    "assessment.write.parquet(\"/tmp/assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the raw assessment data as string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assessment.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Spark Infered Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter the assessment json file by mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/session.py:351: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "extracted_assessment = assessment.rdd.map(lambda x: json.loads(x.value)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is the schema infered by spark. The json extraction based on the infered schema is created as a Spark TempTable of __assessment__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_assessment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_assessment.registerTempTable('assessment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Forced Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some columns included in the spark infered schema in previous secion are not in need of further queries work. So a forced schema is created to only include the information of users, exams' identities (*user_exam_id*, *base_exam_id*, *exam_name*, *keen_id*,etc.) and assessment performance (*incomplete*,*submitted*,*incorrect*, etc.). This schema named as __final_schema__ is created as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_schema = StructType(\n",
    "    [StructField('user_exam_id', StringType(), True),\n",
    "     StructField('base_exam_id', StringType(), True),\n",
    "     StructField('keen_id', StringType(), True),\n",
    "     StructField('exam_name', StringType(), True),\n",
    "     StructField('certification', StringType(), True),\n",
    "     StructField('keen_timestamp', StringType(), True),\n",
    "     StructField('sequences',StructType(\n",
    "         [StructField('counts',StructType([\n",
    "             StructField('incomplete',IntegerType(), True),\n",
    "             StructField('submitted',IntegerType(), True),\n",
    "             StructField('incorrect',IntegerType(), True),\n",
    "             StructField('all_correct',StringType(), True),\n",
    "             StructField('correct',IntegerType(), True),\n",
    "             StructField('total',IntegerType(), True),\n",
    "             StructField('unanswered',IntegerType(), True)\n",
    "         ]))]))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The json extraction based on the __final_schema__ is saved to HDFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_extracted_assessment = assessment.rdd.map(lambda x: json.loads(x.value)).toDF(schema=final_schema)\n",
    "select_extracted_assessment.write.parquet(\"/tmp/select_extracted_assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And a spark TempTable is created as __select_assessment__ for Spark SQL in the following section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_extracted_assessment.registerTempTable('select_assessment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the schema of extraction  works as design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_exam_id: string (nullable = true)\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- sequences: struct (nullable = true)\n",
      " |    |-- counts: struct (nullable = true)\n",
      " |    |    |-- incomplete: integer (nullable = true)\n",
      " |    |    |-- submitted: integer (nullable = true)\n",
      " |    |    |-- incorrect: integer (nullable = true)\n",
      " |    |    |-- all_correct: string (nullable = true)\n",
      " |    |    |-- correct: integer (nullable = true)\n",
      " |    |    |-- total: integer (nullable = true)\n",
      " |    |    |-- unanswered: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_extracted_assessment.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By now, the extracted data is shown as below, which is ready for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+\n",
      "|        user_exam_id|        base_exam_id|             keen_id|           exam_name|certification|    keen_timestamp|           sequences|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+\n",
      "|6d4089e4-bde5-4a2...|37f0a30a-7464-11e...|5a6745820eb8ab000...|Normal Forms and ...|        false| 1516717442.735266|[[1,4,1,false,2,4...|\n",
      "|2fec1534-b41f-441...|37f0a30a-7464-11e...|5a674541ab6b0a000...|Normal Forms and ...|        false| 1516717377.639827|[[2,4,1,false,1,4...|\n",
      "|8edbc8a8-4d26-429...|4beeac16-bb83-4d5...|5a67999d3ed3e3000...|The Principles of...|        false| 1516738973.653394|[[0,4,1,false,3,4...|\n",
      "|c0ee680e-8892-4e6...|4beeac16-bb83-4d5...|5a6799694fc7c7000...|The Principles of...|        false|1516738921.1137421|[[2,4,0,false,2,4...|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_extracted_assessment.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying Data with Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 How many assesstments are in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall, there are 3280 assessments in the dataset. However, there are some duplicate records in the dataset. According to the *keen_id* and *user_exam_id*, there are 3242 distinct assesstments in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|dateset_size|\n",
      "+------------+\n",
      "|        3280|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(keen_id) dateset_size from select_assessment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|keen_id_num|\n",
      "+-----------+\n",
      "|       3242|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct keen_id) as keen_id_num from select_assessment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|user_exam_id_num|\n",
      "+----------------+\n",
      "|            3242|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct user_exam_id) as user_exam_id_num from select_assessment\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, there are 103 distinct exam names  in this dataset. While, there is 107 distinct exam ids in this dataset, which means some courses are of multiple exam IDs. In other words, there are multiple exam versions for some courses. Query below shows that courses like Introduction to Python, Being a Better Inrovert, Great Bash and Architectural Considerations for Hadoop Applications are of 2 exam IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|exam_num|\n",
      "+--------+\n",
      "|     103|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct exam_name) as exam_num from select_assessment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|base_exam_id_num|\n",
      "+----------------+\n",
      "|             107|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct base_exam_id) as base_exam_id_num from select_assessment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------+\n",
      "|ID_num_exam|exam_name                                           |\n",
      "+-----------+----------------------------------------------------+\n",
      "|2          |Introduction to Python                              |\n",
      "|2          |Being a Better Introvert                            |\n",
      "|2          |Great Bash                                          |\n",
      "|2          |Architectural Considerations for Hadoop Applications|\n",
      "|1          |Learning Apache Hadoop                              |\n",
      "|1          |Learning C# Best Practices                          |\n",
      "|1          |Introduction to Java 8                              |\n",
      "|1          |Introduction to Architecting Amazon Web Services    |\n",
      "|1          |Learning Spring Programming                         |\n",
      "|1          |Learning iPython Notebook                           |\n",
      "+-----------+----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct base_exam_id) as ID_num_exam, exam_name from select_assessment group by exam_name order by ID_num_exam DESC\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 How many people took Learning Git?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There were 390 people taking Learning Git, according to the distinct *user_exam_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|People_Taking_Learning_Git|\n",
      "+--------------------------+\n",
      "|                       390|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct user_exam_id) as People_Taking_Learning_Git from select_assessment WHERE exam_name = 'Learning Git'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 What is the least common course taken? And the most common?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The least common courses are Learning to Visualize Data with D3.js, Native Web Apps for Android, Nulls, Three-valued Logic and Missing Information and The Closed World Assumption. There was only 1 person taking these courses, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------+\n",
      "|Popularity|exam_name                                          |\n",
      "+----------+---------------------------------------------------+\n",
      "|1         |Learning to Visualize Data with D3.js              |\n",
      "|1         |Native Web Apps for Android                        |\n",
      "|1         |Nulls, Three-valued Logic and Missing Information  |\n",
      "|1         |Operating Red Hat Enterprise Linux Servers         |\n",
      "|2         |The Closed World Assumption                        |\n",
      "|2         |Client-Side Data Storage for Web Developers        |\n",
      "|2         |Arduino Prototyping Techniques                     |\n",
      "|2         |Understanding the Grails 3 Domain Model            |\n",
      "|2         |Hibernate and JPA Fundamentals                     |\n",
      "|2         |What's New in JavaScript                           |\n",
      "|2         |Learning Spring Programming                        |\n",
      "|3         |Mastering Web Views                                |\n",
      "|3         |Using Web Components                               |\n",
      "|3         |Service Based Architectures                        |\n",
      "|3         |Getting Ready for Angular 2                        |\n",
      "|3         |Building Web Services with Java                    |\n",
      "|4         |View Updating                                      |\n",
      "|4         |Using Storytelling to Effectively Communicate Data |\n",
      "|5         |An Introduction to Set Theory                      |\n",
      "|5         |Example Exam For Development and Testing oh yeahsdf|\n",
      "+----------+---------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(user_exam_id) as Popularity, exam_name from select_assessment group by exam_name order by popularity\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most common course is Learning Git. There were 390 people taking this course according to the *user_exam_id* and *keen_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------------+\n",
      "|Popularity|exam_name                                                  |\n",
      "+----------+-----------------------------------------------------------+\n",
      "|390       |Learning Git                                               |\n",
      "|162       |Introduction to Python                                     |\n",
      "|158       |Introduction to Java 8                                     |\n",
      "|156       |Intermediate Python Programming                            |\n",
      "|128       |Learning to Program with R                                 |\n",
      "|119       |Introduction to Machine Learning                           |\n",
      "|109       |Software Architecture Fundamentals Understanding the Basics|\n",
      "|85        |Learning Eclipse                                           |\n",
      "|83        |Beginning C# Programming                                   |\n",
      "|80        |Learning Apache Maven                                      |\n",
      "+----------+-----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct user_exam_id) as Popularity, exam_name from select_assessment group by exam_name order by popularity DESC\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------------+\n",
      "|Popularity|exam_name                                                  |\n",
      "+----------+-----------------------------------------------------------+\n",
      "|390       |Learning Git                                               |\n",
      "|162       |Introduction to Python                                     |\n",
      "|158       |Introduction to Java 8                                     |\n",
      "|156       |Intermediate Python Programming                            |\n",
      "|128       |Learning to Program with R                                 |\n",
      "|119       |Introduction to Machine Learning                           |\n",
      "|109       |Software Architecture Fundamentals Understanding the Basics|\n",
      "|85        |Learning Eclipse                                           |\n",
      "|83        |Beginning C# Programming                                   |\n",
      "|80        |Learning Apache Maven                                      |\n",
      "+----------+-----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct keen_id) as Popularity, exam_name from select_assessment group by exam_name order by popularity DESC\").show(10, False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
